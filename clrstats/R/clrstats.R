# MagellanMapper stats in R
# Author: David Young, 2018, 2021

# library to avoid overlapping text labels
#install.packages("devtools")
#library("devtools")
#install_github("JosephCrispell/basicPlotteR")

# To run stats:
# - Select profiles in runStats
# - Start R in clrstats folder
# - Load source: "devtools::load_all(".")"
# - Run stats: "runStats()"

# stat processing types
kStatTypes <- c(
  "default", "corr", "norm"
)

# statistical models
kModel <- c(
  "logit", "linregr", "gee", "logit.ord", "ttest",
  "wilcoxon", "ttest.paired", "wilcoxon.paired", "fligner", "basic",
  "diff.mean")

# measurements, which correspond to columns in main data frame
kMeas <- c(
  "Volume", "Density", "Nuclei", "VarIntensity",
  "VarNuclei", "EdgeDistSum", "EdgeDistMean", "DSC_atlas_labels_hemisphere",
  "Compactness", "VarIntensBorder", "VarIntensMatch", "VarIntensDiff",
  "CoefVarIntens", "CoefVarNuc", "MeanIntensity", "MeanNuclei",
  "Intensity", "DSC", "Smoothing_quality", "VolDSC",
  "NucDSC", "VolOut", "NucOut", "NucCluster",
  "NucClusNoise")

# named list to convert measurement columns to display names, consisting 
# of lists of titles/labels and measurement units
kMeasNames <- setNames(
  list(list("Edge Match (Within-Region Intensity Variation)", "SD size"), 
       list("Edge Match (Within-Region Nuclei Variation)", "SD size"),
       list("Label Edge Distances to Anatomical Edges", bquote(list(mu*"m"))), 
       list("Label Edge Distances to Anatomical Edges (Mean)", 
            bquote(list(mu*"m"))), 
       list(paste("Labeled Hemisphere Atlas and Labels Overlay", 
                  "(Dice Similarity Coefficient)"), NULL), 
       list("Region Homogeneity (Core-Periphery Variation Match)", 
            "SD size difference"), 
       list("Edge Noise (Core-Periphery Variation Difference)", 
            "SD size difference"), 
       list("Edge Match (Within-Region Intensity Variation)", 
            "Coefficient of variation"), 
       list("Edge Match (Within-Region Nuclei Variation)", 
            "Coefficient of variation"), 
       list("Atlas and Labels Overlay (Dice Similarity Coefficient)", NULL)), 
  c(kMeas[c(4:8, 11:14, 18)]))

# ordered genotype levels
kGenoLevels <- c(0, 0.5, 1)

# regions to ignore (eg duplicates)
kRegionsIgnore <- c(15564)


# File Paths

# paths to files of raw metric values from MagellanMapper
kStatsFilesIn <- c(
  "vols_by_sample.csv", "vols_by_sample_levels.csv", 
  "vols_by_sample_summary.csv", "dsc_summary.csv", 
  "compactness_summary.csv", "compactness_summary_stats.csv", 
  "reg_stats_melted.csv", "smoothing_gausVopen.csv",
  "vols_by_sample_compare.csv", "vols_by_sample_compare_levels.csv"
)
kStatsPathOut <- "vols_stats" # output stats

# configurable environment
config.env <- new.env()


fitModel <- function(model, vals, genos, sides, ids=NULL) {
  # Fit data with the given regression model.
  #
  # Args:
  #   model: Model to use, corresponding to one of kModel.
  #   vals: Main independent variable.
  #   genos: Genotypes vector.
  #   sides: Vector corresponding to the side of vals, eg left or right.
  #   ids: Vector of sample IDs; defaults to NULL.
  #
  # Returns:
  #   Coefficients of the summary statistics. The first row of coefficients 
  #   is removed if it is a non-intercept row. The colums are assumed to have 
  #   effect size in the 2nd column and p-value in the 4th.
  
  result <- NULL
  col.effect <- "Estimate"
  num.sides <- length(unique(sides))
  if (model == kModel[1]) {
    # logistic regression
    if (num.sides > 1) {
      fit <- glm(genos ~ vals * sides, family=binomial)
    } else {
      fit <- glm(genos ~ vals, family=binomial)
    }
    result <- summary.glm(fit)$coefficients
    # remove first ("non-intercept") row
    result <- result[-(1:1), ]
  } else if (model == kModel[2]) {
    # linear regression
    # TODO: see whether need to factorize genos
    fit <- lm(vals ~ genos * sides)
    result <- summary.lm(fit)$coefficients
    # remove first ("non-intercept") row
    result <- result[-(1:1), ]
  } else if (model == kModel[3]) {
    # generalized estimating equations
    # TODO: fix model prob "fitted value very close to 1" error
    fit <- gee::gee(
      genos ~ vals * sides, ids, corstr="exchangeable", family=binomial())
    result <- summary(fit)$coefficients
  } else if (model == kModel[4]) {
    # ordered logistic regression
    vals <- scale(vals)
    genos <- factor(genos, levels=kGenoLevels)
    fit <- tryCatch({
      fit <- MASS::polr(genos ~ vals * sides, Hess=TRUE)
      result <- coef(summary(fit))
      # calculate p-vals and incorporate into coefficients
      p.vals <- pnorm(abs(result[, "t value"]), lower.tail=FALSE) * 2
      result <- cbind(result, "p value"=p.vals)
    },
      error=function(e) {
        print(paste("Could not calculate ordered logistic regression", 
              e, "skipping"))
      }
    )
  } else {
    cat("Sorry, model", model, "not found\n")
  }
  
  # basic stats data frame in format for filterStats
  coef.tab <- setupBasicStats()
  effect <- result[[col.effect]]
  coef.tab$Value <- c(effect)
  stderr <- "Std. Error"
  if (is.element(stderr, names(result))) {
    # use SEM for the "CI" values
    ci <- result[[stderr]]
    coef.tab$CI.low <- c(effect - ci)
    coef.tab$CI.hi <- c(ci - effect)
  }
  coef.tab$P <- c(result[4])
  coef.tab$N <- c(length(vals))
  print(coef.tab)
  return(coef.tab)
}

meansModel <- function(vals, conditions, model, paired=FALSE, reverse=FALSE) {
  # Test for differences of means.
  #
  # Args:
  #   vals: List of values to compare.
  #   conditions: List assumed to have at least two conditions by which to 
  #     group; only the first two sorted conditions will be used, and all 
  #     other conditions will be ignored. For paired tests, number of 
  #     values per condition is assumed to be the same. Conditions are 
  #     sorted alphabetically, and effect sizes are generated by taking 
  #     values from the second minus that of the first condition.
  #   model: One of kModels to apply, which should be a mean test such as 
  #     "ttest" or "wilcoxon".
  #   paired: True for paired test; defaults to FALSE.
  #   reverse: True to reverse the order of sorted conditions; defaults 
  #     to FALSE.
  #
  # Returns:
  #   Data frame of with p-value and mean of differences columns.
  
  # require at least 2 condition to compare, sorting in direction based 
  # on reverse argument
  conditions.unique <- sort(unique(conditions), decreasing=reverse)
  if (length(conditions.unique) < 2) {
    cat("need at least 2 conditions, cannot compare means\n")
    return(NULL)
  }
  
  # build lists of value vectors for each condition
  val.conds <- list()
  num.per.cond <- NULL
  for (i in seq_along(conditions.unique)) {
    val.conds[[paste0("cond", i)]] <- vals[conditions == conditions.unique[i]]
    num.per.cond <- length(val.conds[[i]])
    if (is.element(model, kModel[c(5, 7)]) & num.per.cond <= 1) {
      # T-tests requires >= 2 values
      cat("<2 values for at least one condition, cannot calculate stats\n")
      return(NULL)
    }
  }
  
  effect <- NULL
  effect.raw <- NULL
  ci <- NULL
  result <- tryCatchLog::tryCatchLog({
    if (model == kModel[5] | model == kModel[7]) {
      # Student's t-test
      result <- t.test(val.conds[[2]], val.conds[[1]], paired=paired)
      
      # calculate Cohen's d for standardized effect
      
      # # convert lists of potentially different sizes to df in long format
      # df <- stack(val.conds)
      # print(df)
      
      # # effect size using rstatix; CI does not appear to be working
      # eff <- rstatix::cohens_d(df, values ~ ind, paired=paired)
      # print(eff)
      
      # effect size using effectsize
      # eff <- effectsize::cohens_d(values ~ ind, data=df, paired=paired)
      eff <- effectsize::cohens_d(val.conds[[2]], val.conds[[1]], paired=paired)
      print(eff)
      # print(effectsize::interpret_cohens_d(eff))
      effect <- eff$Cohens_d
      ci <- c(eff$CI_low, eff$CI_high)
      
      # get raw effect; get diff if multiple vals
      effect.raw <- result[["estimate"]]
      if (length(effect.raw) > 1) {
        effect.raw <- -diff(effect.raw)
      }
  
    } else if (model == kModel[6] | model == kModel[8]) {
      # Wilcoxon test (Mann-Whitney if not paired)
      result <- wilcox.test(
        val.conds[[2]], val.conds[[1]], paired=paired, conf.int=TRUE)
  
      # replace the main effect with a standardized effect size, given as
      # z / sqrt(N), where N = number of pairs
      effect.raw <- result[["estimate"]]
      effect <- rcompanion::wilcoxonZ(
        val.conds[[2]], val.conds[[1]], paired=paired) / sqrt(num.per.cond)
      cat("Wilcoxon estimate: ", effect.raw, ", standardized effect: ",
          effect, "\n", sep="")
  
    } else if (model == kModel[9]) {
      # Fligner-Killen test of variance
      result <- fligner.test(vals, conditions)
      effect <- result[["statistic"]]
  
    } else if (model == kModel[11]) {
      # difference of means
      effect <- mean(
        val.conds[[2]], na.rm=TRUE) - mean(val.conds[[1]], na.rm=TRUE)
      result <- list(estimate=effect, p.value=NA)
  
    } else {
      cat("Sorry, model", model, "not found\n")
    }
    
    # return result from try block
    print(result)
    result
  }, error=function(e) {
    message("Unable to generate stat, skipping")
    return(NULL)
  }, finally={
  }, include.full.call.stack=FALSE, include.compact.call.stack=FALSE)
  
  # return if no stats
  if (is.null(result)) return(NULL)
  
  # basic stats data frame in format for filterStats
  coef.tab <- setupBasicStats()
  coef.tab$Value <- effect
  coef.tab$Value.raw <- effect
  if (!is.null(effect.raw)) {
    # store raw effect if it was standardized; otherwise, leave same as effect
    coef.tab$Value.raw <- effect.raw
  }
  
  if (is.element("conf.int", names(result))) {
    # store confidence intervals from result as both standardized and raw CIs
    ci.raw <- result$conf.int
    coef.tab$CI.low.raw <- ci.raw[1]
    coef.tab$CI.hi.raw <- ci.raw[2]
    coef.tab$CI.low <- coef.tab$CI.low.raw
    coef.tab$CI.hi <- coef.tab$CI.hi.raw
  }
  if (!is.null(ci)) {
    # store standardized confidence intervals
    coef.tab$CI.low <- ci[[1]]
    coef.tab$CI.hi <- ci[[2]]
  }
  
  coef.tab$P <- result$p.value
  coef.tab$N <- num.per.cond
  print(coef.tab)
  return(coef.tab)
}

setupBasicStats <- function() {
  # Setup a data frame for basic stats.
  #
  # Returns:
  #   Data frame with columns for basic statistics such as mean and 
  #   confidence intervals and a single empty row.
  
  cols <- c("N", "Value", "CI.low", "CI.hi", "Value.raw", "CI.low.raw", "CI.hi.raw", "P")
  coef.tab <- data.frame(matrix(nrow=1, ncol=length(cols)))
  names(coef.tab) <- cols
  rownames(coef.tab) <- "vals"
  return(coef.tab)
}

setupPairing <- function(df.region, col, split.col) {
  # Setup data frame for comparing paired groups.
  #
  # Assume that the data frame has been sorted by sample so that samples 
  # will be matched after splitting by split.col.
  #
  # Args:
  #   df.region: Data frame only the samples to compare.
  #   col: Column name of values to compare.
  #   split.col: Column name by which to split samples into groups, assumed 
  #     to have at least two groups within column.
  #
  # Returns:
  #   New data frame filtering out any pair that lacks positive values for 
  #   both members of the pair, or NULL if unable to pair.
  
  # require at least 2 condition to compare
  conditions <- df.region[[split.col]]
  conditions.unique <- sort(unique(conditions))
  if (length(conditions.unique) < 2) {
    cat("need at least 2 conditions, cannot set up pairing\n")
    return(NULL)
  }
  vals <- df.region[[col]]
  
  # build up nonnan mask to filter out any pairs with any zero vals
  nonnan <- NULL
  num.per.cond <- NULL
  for (cond in conditions.unique) {
    val.cond <- vals[cond == conditions]
    # TODO: look for all non-zero, not just pos vals
    nonnan.cond <- !is.nan(val.cond)
    if (is.null(nonnan)) {
      nonnan <- nonnan.cond
    } else if (num.per.cond != length(val.cond)) {
      cat("unequal number of values per conditions, cannot match pairs\n")
      return(NULL)
    } else {
      nonnan <- nonnan & nonnan.cond
    }
    num.per.cond <- length(val.cond)
  }
  
  df.filtered <- NULL
  num.nonnan <- sum(nonnan)
  if (num.nonnan == 0) {
    cat("no non-zero values would remain after filtering\n")
    return(NULL)
  } else if (num.nonnan < num.per.cond) {
    # build new data frame with each condition filtered by mask
    for (cond in conditions.unique) {
      df.nonnan <- df.region[cond == conditions, ][nonnan, ]
      if (is.null(df.filtered)) {
        df.filtered <- df.nonnan
      } else {
        df.filtered <- rbind(df.filtered, df.nonnan)
      }
    }
  } else {
    # no filtering required
    df.filtered <- df.region
  }
  return(df.filtered)
}

statsByRegion <- function(df, col, model, split.by.side=TRUE, 
                          regions.ignore=NULL, cond=NULL, group.col=NULL) {
  # Calculate statistics given by region for columns starting with the given 
  # string using the selected model.
  #
  # NaN values will be ignored. If all values for a given vector are NaN, 
  # statistics will not be computed.
  #
  # Groups for comparison will be defined by "group.col" if set. If not,
  # non-paired stats default to compare groups specified in the "Geno" column,
  # while paired stats compare based on the "Condition" column.
  #
  # Args:
  #   df: Data frame with at least columns for "Sample" and "Region".
  #   col: Column from which to find main stats.
  #   model: Model to use, corresponding to one of kModel.
  #   split.by.side: True to keep data split by sides, False to combine 
  #     corresponding regions from opposite sides into single regions; 
  #     defaults to True.
  #   regions.ignore: Vector of regions to ignore; default to NULL.
  #   cond: Filter df to keep only this condition; defaults to NULL.
  #   group.col: Name of group column; defaults to NULL, which uses "Condition"
  #     for means models and "Geno" otherwise.
  
  mean_model_inds <- c(5:9, 11)
  if (is.null(group.col)) {
    # set up default group column name
    if (is.element(model, kModel[mean_model_inds])) {
      # means models default to splitting by condition
      group.col <- "Condition"
    } else {
      # default to splitting by genotype
      group.col <- "Geno"
    }
    cat("\ndefaulting group column to", group.col, "\n")
  }
  
  # find all regions
  regions <- unique(df$Region)
  # regions <- c(15565, 15566) # TESTING: select region(s)
  cols <- c("Region", "Stats", "Volume", "Nuclei")
  stats <- data.frame(matrix(nrow=length(regions), ncol=length(cols)))
  names(stats) <- cols
  if (!is.null(cond)) {
    # filter by the given condition
    df <- df[df$Condition == cond, ]
  }
  # use original order of appearance in Condition column to sort each 
  # region since order may change from region to region
  cond.unique <- unique(df$Condition)
  regions.ignored <- vector()
  
  for (i in seq_along(regions)) {
    region <- regions[i]
    if (!is.null(regions.ignore) & is.element(region, kRegionsIgnore)) next
    
    # filter data frame for the given region and get mask to filter out 
    # NaNs and 0's as they indicate that the label for the region was suppressed
    df.region <- df[df$Region == region, ]
    nonnan <- !is.nan(df.region[[col]])
    stats$Region[i] <- as.character(region)
    
    if (any(nonnan)) {
      cat("\nRegion", region, "\n")
      df.region.nonnan <- df.region
      split.col <- NULL
      paired <- is.element(model, kModel[7:9])
      
      if (!paired) {
        # filter each column within region for rows with non-zero values
        df.region.nonnan <- df.region.nonnan[nonnan, ]
        if (is.null(df.region.nonnan)) next
      }
      if (is.element(model, kModel[mean_model_inds])) {
        # filter for means tests, which compare groups specified in group.col
        # TODO: reconsider aggregating sides but need way to properly
        # average variations in a weighted manner
        split.col <- group.col
        if (paired) {
          # sort by sample and condition, matching saved condition order,
          # split by condition, and filter out pairs where either sample
          # has a zero value
          #print(df.region.nonnan)
          df.region.nonnan <- df.region.nonnan[
            order(df.region.nonnan$Sample,
                  match(df.region.nonnan[[split.col]], cond.unique)), ]
          df.region.nonnan <- setupPairing(df.region.nonnan, col, split.col)
          if (is.null(df.region.nonnan)) next
        }
      }
      #print(df.region.nonnan)
      vals <- df.region.nonnan[[col]]
      
      # apply stats and store in stats data frame, using list to allow 
      # arbitrary size and storing mean volume as well
      coef.tab <- NULL
      if (is.element(model, kModel[mean_model_inds])) {
        # means tests
        coef.tab <- meansModel(
          vals, df.region.nonnan[[group.col]], model, paired, 
          config.env$ReverseConditions)
        
      } else if (model == kModel[10]) {
        # basic stats
        coef.tab <- setupBasicStats()
        
        # show histogram to check for parametric distribution
        #histogramPlot(vals, title, meas)
        
      } else {
        # regression tests
        genos <- df.region.nonnan[[group.col]]
        sides <- df.region.nonnan$Side
        ids <- df.region.nonnan$Sample
        coef.tab <- fitModel(model, vals, genos, sides, ids)
      }
      
      if (!is.null(coef.tab)) {
        # collect stats, taking means for weightings
        stats$Stats[i] <- list(coef.tab)
        stats$Volume[i] <- mean(df.region.nonnan$Volume)
        stats$Nuclei[i] <- mean(df.region.nonnan$Nuclei)
      }
      
      # construct title from region identifiers and capitalize first letter
      df.jitter <- df.region.nonnan
      region.name <- df.region.nonnan$RegionName[1]
      if (is.na(region.name)) {
        title <- region
        if (is.factor(title) | is.integer(title)) title <- as.character(title)
      } else {
        title <- paste0(region.name, " (", region, ")")
      }
      substring(title, 1, 1) <- toupper(substring(title, 1, 1))
      
      # plot individual values grouped by genotype and selected column
      if (!split.by.side) {
        df.jitter <- aggregate(
          cbind(Volume, Nuclei) ~ Sample + group.col, df.jitter, sum)
        df.jitter$Density <- df.jitter$Nuclei / df.jitter$Volume
        print(df.jitter)
      }
      group.col.jitter <- group.col
      if (!is.null(split.col) && group.col.jitter == split.col) {
        # paired stats default to using "Condition" for both group and split
        # columns, so need to change if the same
        # TODO: allow split.col to be ignored
        group.col.jitter <- "Geno"
      }
      # TODO: set up groups and generate stats outside of jitter plots
      stats.group <- jitterPlot(
        df.jitter, col, title, group.col.jitter, split.by.side, split.col, 
        paired, config.env$SampleLegend, config.env$PlotSize, 
        axes.in.range=config.env$Axes.In.Range, 
        summary.stats=config.env$SummaryStats, 
        save=config.env$JitterPlotSave, sort.groups=config.env$Sort.Groups,
        show.labels=config.env$JitterLabels)
      
      # add mean, median, and CI for each group to stats data frame
      names <- stats.group[[1]]
      for (j in seq_along(names)) {
        stats[i, paste0(names[j], ".n")] <- stats.group[[2]][j]
        stats[i, paste0(names[j], ".mean")] <- stats.group[[3]][j]
        stats[i, paste0(names[j], ".med")] <- stats.group[[4]][j]
        stats[i, paste0(names[j], ".sd")] <- stats.group[[5]][j]
        stats[i, paste0(names[j], ".ci")] <- stats.group[[6]][j]
      }
    } else {
      # ignore region if all values 0, leaving entry for region as NA and 
      # grouping output for empty regions to minimize console output; 
      # TODO: consider grouping into list and displaying only at end
      regions.ignored <- append(regions.ignored, region)
    }
  }
  if (length(regions.ignored) > 0) {
    cat("\nno non-zero samples found for these regions:")
    print(regions.ignored)
  }
  return(stats)
}

histogramPlot <- function(vals, title, meas) {
  # Plot histogram and save to file.
  #
  # Args:
  #   vals: Values to plot.
  #   title: Title for plot.
  #   meas: Measurement to include in filename
  
  # may need to tweak width divisor to fit exactly in fig
  hist(vals, main=strwrap(title, width=dev.size("px")[1]/10))
  dev.print(
    pdf, file=paste0(
      "../plot_histo_", meas, "_", gsub("/| ", "_", title), ".pdf"))
}

filterStats <- function(stats, corr=NULL) {
  # Filter regional statistics to remove \code{NA}s and gather the most 
  # pertinent statistical values.
  #
  # Args:
  #   stats: Data frame generated by \code{\link{statsByRegion}}.
  #   corr: String of correction type to pass to p.adjust.
  #
  # Returns:
  #   Filtered data frame with columns for Region, Effect, and p.
  
  non.na <- !is.na(stats$Stats)
  stats.filt <- stats[non.na, ]
  if (length(stats.filt$Stats) < 1) return(NULL)
  
  filtered <- NULL
  interactions <- NULL
  offset <- 0 # number of columns ahead of coefficients
  
  # get names and mean and CI columns
  cols.names <- names(stats.filt)
  cols.means.cis <- c(
    cols.names[grepl(".n", cols.names)], 
    cols.names[grepl(".mean", cols.names)], 
    cols.names[grepl(".med", cols.names)], 
    cols.names[grepl(".sd", cols.names)], 
    cols.names[grepl(".ci", cols.names)])
  
  # build data frame for pertinent coefficients from each type of main 
  # effect or interaction
  stats.coef <- stats.filt$Stats[1][[1]]
  interactions <- gsub(":", ".", rownames(stats.coef))
  cols <- list("Region", "Volume", "Nuclei")
  cols.orig <- cols # points to original vector if it is mutated
  offset <- length(cols)
  cols.suffixes <- c(
    ".n", ".effect", ".ci.low", ".ci.hi", ".effect.raw", ".ci.low.raw", ".ci.hi.raw", ".p", ".pcorr",
    ".logp")
  for (interact in interactions) {
    for (suf in cols.suffixes) {
      cols <- append(cols, paste0(interact, suf))
    }
  }
  filtered <- data.frame(matrix(nrow=nrow(stats.filt), ncol=length(cols)))
  names(filtered) <- cols
  for (col in cols.orig) {
    # copy base columns
    filtered[[col]] <- stats.filt[[col]]
  }
  num.stat.cols <- length(names(stats.coef))
  
  for (i in 1:nrow(stats.filt)) {
    if (is.na(stats.filt$Stats[i])) next
    # get coefficients, stored in one-element list
    stats.coef <- stats.filt$Stats[i][[1]]
    num.stats <- ncol(stats.coef)
    for (j in seq_along(interactions)) {
      # insert effect, p-value, and -log(p) after region name for each 
      # main effect/interaction, ignoring missing rows
      if (nrow(stats.coef) >= j) {
        start <- offset + (num.stats + 1) * (j - 1) + 1
        filtered[i, start:(start+num.stat.cols)] <- stats.coef[j, ]
      }
    }
    for (col in cols.means.cis) {
      # add all mean and CI column values
      filtered[i, col] <- stats.filt[i, col]
    }
  }
  
  for (interact in interactions) {
    # only correct/adjust means stats with >= 2 vals
    filt.n <- filtered[[paste0(interact, ".n")]]
    mask.corr <- filt.n > 1 & !is.na(filt.n) # eg NA from basic stats
    num.regions <- nrow(filtered[mask.corr, ])
    col.for.log <- paste0(interact, ".pcorr")
    if (!is.null(corr) && num.regions > 0) {
      # apply correction based on number of comparisons
      col <- paste0(interact, ".p")
      cat("correcting", col, "by", corr, "for", num.regions, "regions\n")
      filtered[mask.corr, col.for.log] <- p.adjust(
        filtered[mask.corr, col], method=corr, n=num.regions)
    }
    # calculate -log-p values
    filtered[[paste0(interact, ".logp")]] <- -1 * log10(filtered[[col.for.log]])
  }
  
  return(filtered)
}

#' Calculate volumetric stats from the given CSV file.
#'
#' @param df Volume stats data frame, assumed to be generated by
#'   \code{magmap.stats.regions_to_pandas} Python function.
#' @param path.out Path to output CSV file.
#' @param meas Column from which to generate stats, which should be one of 
#'   [kMeas].
#' @param model Model type to use for stats, which should be one of 
#'   [kModel].
#' @param region.ids Data frame of region IDs to merge by the "Region"
#'   column. Defaults to NULL, in which case region data will be merged
#'   from `df` if available.
#' @param split.by.side True to plot separate sub-scatter plots for each 
#'   region by side; defaults to True.
#' @return Filtered data frame from [filterStats].
calcVolStats <- function(
    df, path.out, meas, model, region.ids=NULL, split.by.side=TRUE, corr=NULL) {
  
  if (!is.element(meas, names(df))) {
    cat(paste(meas, "not found in data frame", "\n"))
    return(NULL)
  }
  
  # convert summary regions into "Mus Musculus" (ID 15564), the 
  # over-arching parent, which will be skipped if in kRegionsIgnore
  region.all <- df$Region == "all"
  if (any(region.all)) {
    df$Region <- as.integer(df$Region)
    df$Region[region.all] <- 15564
  }
  
  if (is.null(region.ids)) {
    # get regions columns from main df
    cols <- c("Region", "RegionName", "RegionAbbr", "Level")
    cols <- cols[cols %in% colnames(df)]
    region.ids <- unique(df[cols])
  } else {
    # merge in region names based on matching IDs
    df <- merge(df, region.ids, by="Region", all.x=TRUE)
  }
  #print.data.frame(df)
  print(str(df)) # show data frame structure
  cat("\n\n")
  
  # calculate stats, filter out NAs and extract effects and p-values
  stats <- statsByRegion(
    df, meas, model, split.by.side=split.by.side,
    regions.ignore=config.env$Regions.Ignore,
    cond=config.env$Condition, group.col=config.env$GroupCol)
  stats.filtered <- filterStats(stats, corr=corr)
  if (is.null(stats.filtered)) {
    message("No stats were generated. Please check input file, regions,",
            "and stats settings.\n")
  } else {
    if (!is.null(region.ids)) {
      # merge in regions table
      stats.filtered <- merge(
        region.ids, stats.filtered, by="Region", all.y=TRUE)
    }
    print(stats.filtered)
    message("Writing stats to: ", path.out)
    write.csv(stats.filtered, path.out)
  }
  
  return(stats.filtered)
}

calcCorr <- function(path.in, cols, plot.size=c(5, 7), suffix=NULL) {
  # Calculate correlation coefficient matrix for columns in a data frame 
  # and plot with significance.
  #
  # Args:
  #   path.in: CSV input path for data frame.
  #   cols: Vector of columns for which to build correlation matrix.
  #   plot.size: Vector of width, height for exported plot; defaults to 
  #     c(5, 7).
  #   suffix: String of output path suffix inserted before the stat type;
  #     defaults to NULL.
  
  # load CSV to data frame, calculate correlation coefficient matrix, 
  # and save correlations and p-vals to CSV
  cat("\nloading", path.in, "\n")
  df <- read.csv(path.in)
  base.path <- tools::file_path_sans_ext(basename(path.in))
  corr <- Hmisc::rcorr(as.matrix(df[, cols]), type="spearman")
  print(corr)
  if (!is.null(suffix)) {
    base.path <- paste(base.path, suffix, sep="_")
  }
  out.path <- paste0("../", base.path, "_corr")
  write.csv(corr$r, paste0(out.path, "_r.csv"))
  write.csv(corr$P, paste0(out.path, "_p.csv"))
  
  # plot correlation matrix and save to PDF
  corrplot::corrplot(
    corr$r, method="circle", order="hclust", p.mat=corr$p, 
    sig.level=0.05, insig="p-value")
  plot.path <- paste0("../plot_corr_", base.path, ".pdf")
  dev.print(pdf, width=plot.size[1], height=plot.size[2], file=plot.path)
  cat(paste("Output plot to", plot.path, "\n"))
}

calcNormality <- function(path.in, cols) {
  # Calculate normality statistic using the Shapiro test.
  #
  # Args:
  #   path.in: CSV input path for data frame.
  #   cols: Vector of columns to test normality for each column.
  
  cat("\nloading", path.in, "\n")
  df <- read.csv(path.in)
  for (col in cols) {
    cat("\ntesting normality of col", col, "\n")
    vals <- df[[col]]
    vals <- vals[!is.nan(vals)]
    nor <- shapiro.test(vals)
    print(nor)
  }
}

setupConfig <- function(name=NULL) {
  # Setup configuration environment for the given profile.
  #
  # Args:
  #   name: Name of profile to load. Defaults to NULL, which will initialize  
  #     the environment with default settings.

  loaded <- TRUE
  if (is.null(name)) {
    # initialize environment
    name <- "default"
    config.env$Verbose <- FALSE
    config.env$PlotSize <- c(5, 7)
    config.env$SampleLegend <- FALSE
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[2])
    config.env$Prefix <- ".."
    config.env$Measurements <- kMeas[6]
    config.env$Model <- kModel[8]
    config.env$PlotVolcano <- FALSE
    config.env$VolcanoLabels <- TRUE
    config.env$VolcanoLogX <- TRUE
    config.env$JitterPlotSave <- TRUE
    config.env$JitterLabels <- FALSE
    config.env$Axes.In.Range <- FALSE
    config.env$ReverseConditions <- FALSE
    config.env$SummaryStats <- kSummaryStats[2]
    config.env$GroupCol <- NULL
    config.env$Sort.Groups <- TRUE
    config.env$Condition <- NULL
    config.env$P.Corr <- "bonferroni"
    config.env$ylim <- NULL
    config.env$Regions.Ignore <- NULL # vector or regions to exclude
    config.env$Split.By.Side <- TRUE # FALSE to combine sides
    # region-ID map from MagellanMapper, which should contain all regions including 
    # hierarchical/ontological ones
    config.env$Labels.Path <- "../region_ids.csv"

  } else if (endsWith(name, ".R")) {
    # load a profile file
    if (!file.exists(name)) {
      # fallback to assuming file is in R/profiles folder
      name <- file.path("R", "profiles", name)
    }
    source(name)

    # STAT MODES

  } else if (name == "aba") {
    # multiple distinct atlases
    config.env$SampleLegend <- TRUE
    config.env$Measurements <- kMeas[c(6, 13)]
    config.env$PlotVolcano <- FALSE
    config.env$Regions.Ignore <- kRegionsIgnore
    setupConfig("skinny")
    
  } else if (name == "dsc") {
    # Dice Similarity Coefficient stats for ABA series
    setupConfig("aba")
    setupConfig("square")
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[4])
    config.env$Measurements <- kMeas[8]
    config.env$GroupCol <- "Condition"
    config.env$Regions.Ignore <- NULL
    
  } else if (name == "smoothing") {
    # smoothing quality comparison between Gaussian and opening filter methods
    setupConfig("aba")
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[8])
    config.env$Measurements <- kMeas[19]
    config.env$Regions.Ignore <- NULL
    
  } else if (name == "compactness") {
    # compactness combined jitter plot for ABA series by treating conditions 
    # as separate genotypes of the same "all" region
    setupConfig("aba")
    setupConfig("square")
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[5])
    config.env$Measurements <- kMeas[9]
    config.env$Sort.Groups <- FALSE
    config.env$Regions.Ignore <- NULL
    
  } else if (name == "compactness.stats") {
    # compactness stats for ABA series by treating conditions as different 
    # regions to get corrected p-vals for all "regions"
    setupConfig("compactness")
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[6])
    config.env$JitterPlotSave <- FALSE
    
  } else if (name == "reg") {
    # WT registrations
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[7])
    config.env$Measurements <- kMeas[18]
    config.env$Model <- kModel[10]
    config.env$PlotVolcano <- FALSE
    config.env$Axes.In.Range <- TRUE
    config.env$SummaryStats <- kSummaryStats[1]
    config.env$Sort.Groups <- FALSE
    
  } else if (name == "compare.vol") {
    # basic stats from comparison of two atlases
    setupConfig("aba")
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[10])
    config.env$Model <- kModel[10]
    config.env$PlotVolcano <- FALSE
    config.env$Measurements <- kMeas[20:23]
    
  } else if (name == "nolevels") {
    # input file from drawn labels only, without levels
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[1])
    
  } else if (name == "nojittersave") {
    # plot but don't save jitter plots
    config.env$JitterPlotSave <- FALSE


    # SAMPLE GROUPS

  } else if (name == "wt") {
    # WT samples
    config.env$Measurements <- kMeas[c(1:7, 13:14)]
    config.env$PlotVolcano <- TRUE
    config.env$VolcanoLabels <- FALSE
    config.env$VolcanoLogX <- FALSE
    config.env$Regions.Ignore <- kRegionsIgnore
    
  } else if (name == "intensnuc") {
    # WT intensity and nuclei counts
    setupConfig("wt")
    config.env$StatsPathIn <- file.path("..", kStatsFilesIn[1])
    config.env$Measurements <- kMeas[c(3, 17)]
    
  } else if (name == "wt.test") {
    # WT test
    setupConfig("wt")
    config.env$Measurements <- kMeas[14]
    config.env$SampleLegend <- TRUE
    
  } else if (name == "clustering") {
    # nuclei clustering
    setupConfig("wt")
    config.env$Measurements <- kMeas[24:25]
    
  } else if (name == "geno") {
    # compare across multiple genotypes
    config.env$Measurements <- kMeas[1:3]
    config.env$Model <- kModel[1]
    config.env$PlotVolcano <- TRUE
    config.env$VolcanoLogX <- FALSE
    config.env$Condition <- "smoothed"
    config.env$GroupCol <- "Geno"
    config.env$JitterLabels <- TRUE
    
  } else if (name == "lessstringent") {
    # compare 2 genotypes with slightly less stringent tests
    setupConfig("benjamini.hochman")
    config.env$Model <- kModel[6]
    
  } else if (name == "compare.sex") {
    # compare sex instead of genotype; M vs F unpaired stats
    setupConfig("geno")
    config.env$GroupCol <- "Sex"
    
  } else if (name == "compare.laterality") {
    # compare left/right hemispheres instead of genotype; assumes that
    # the samples will be in the same order after splitting by side;
    # L vs R paired stats
    setupConfig("geno")
    config.env$GroupCol <- "Side"
    config.env$Model <- kModel[8]
    
  } else if (name == "benjamini.hochman") {
    # use Benjamini-Hochman False Discovery Rate instead of Bonferroni
    config.env$P.Corr <- "BH"
    
  } else if (name == "basic.stats") {
    # generate only basic stats
    config.env$Model <- kModel[10]
    config.env$PlotVolcano <- FALSE

  } else if (name == "combine.sides") {
    # combine equal Region values with different Side column values
    config.env$Split.BySide <- FALSE


    # PLOT OPTIONS

  } else if (name == "skinny") {
    # very narrow plots
    config.env$PlotSize <- c(3.5, 7)
    
  } else if (name == "skinny.small") {
    # narrow and short plots
    config.env$PlotSize <- c(3.5, 5)
    
  } else if (name == "square") {
    # square plots
    config.env$PlotSize <- c(7, 7)
    
  } else if (name == "revconds") {
    # reverse the order of conditions in paired stats
    config.env$ReverseConditions <- TRUE
    
  } else {
    loaded <- FALSE
    message("Could not find ", name, " profile to load")
  }
  if (loaded) {
    message(name, " profile loaded")
  }
}

#' Rename a data frame column.
#' 
#' @param df Data frame.
#' @param from Name of column from which to change.
#' @param to Name to which the column will be changed.
#' @return The modified data frame.
renameCol <- function(df, from, to) {
  # ensure that no existing `to` column exists
  region.i <- match(to, colnames(df))
  if (is.na(region.i)) {
    # get the index of the first matching column for the column to change
    region.i <- match(from, colnames(df))
    if (!is.na(region.i)) {
      # rename the column
      colnames(df)[region.i] <- to
    }
  }
  return(df)
}

#' Load data and run full stats.
#'
#' @param path Input data path, typically a spreadsheet such as a CSV file;
#'   defaults to NULL to use ``StatsPathIn`` in the environment profile.
#' @param profiles Profile names, where multiple profiles can be given
#'   separated by commas; defaults to NULL.
#' @param measurements Measurements names, where multiple measurements can be
#'   given separated by commas; defaults to NULL.
#' @param verbose True to show verbose debugging information; defaults to NULL.
#' @param stat.type One of kStatTypes specifying stat processing typest. 
#'   Defaults to NULL to use kStatTypes[1].
#' @param parsed.cli.args Named list of parsed command-line interface
#'   arguments; defaults to NULL.
#' @param model Statistical model to use, which should be one of `kModel`.
#'   Defaults to NULL to use the model in [config.env].
runStats <- function(
    path=NULL, profiles=NULL, measurements=NULL, prefix=NULL, verbose=NULL,
    stat.type=NULL, model=NULL, parsed.cli.args=NULL) {

  if (is.null(stat.type)) {
    message("Running general stats")
  } else {
    message("Running stats of type", stat.type)
  }

  # set up configuration environment based on profile names
  setupConfig()
  if (is.null(profiles)) {
    profiles.split <- c("wt", "basic.stats", "revpairedstats")
  } else {
    profiles.split <- strsplit(profiles, ",")[[1]]
  }
  for (profile in profiles.split) {
    setupConfig(profile)
  }

  if (!is.null(verbose)) {
    # set path prefix
    config.env$Verbose <- verbose
    cat("Set verbose mode to:", config.env$Verbose, "\n")
  }

  if (is.null(path)) {
    # default to use path from profile
    path <- config.env$StatsPathIn
  }

  if (is.null(measurements)) {
    # default to use measurements from profile
    measurements.split <- config.env$Measurements
  } else {
    measurements.split <- strsplit(measurements, ",")[[1]]
  }
  cat("Measurements:", paste(measurements.split), "\n")

  if (!is.null(model)) {
    config.env$Model <- model
    message("Set stats model to: ", config.env$Model)
  }

  if (!is.null(prefix)) {
    # set path prefix
    config.env$Prefix <- prefix
    cat("Outputting files to:", config.env$Prefix, "\n")
  }
  
  if (!is.null(parsed.cli.args)) {
    # apply remaining parsed CLI arguments
    print(parsed.cli.args)
    parsed.names <- names(parsed.cli.args)
    if ("labels" %in% parsed.names) {
      # set labels reference path
      config.env$Labels.Path <- parsed.cli.args$labels
      message("Set region labels reference path to: ", config.env$Labels.Path)
    }
  }
  
  if (config.env$Verbose) {
    cat("Environment settings:\n")
    print(mget(ls(config.env), envir=config.env))
  }

  if (is.null(stat.type) || stat.type == kStatTypes[1]) {
    # default, general stats
    
    # setup measurement and model types
    load.stats <- FALSE # true to load saved stats, only regenerate volcano plots
    
    # set up parameters based on chosen model
    stat <- "vals"
    if (config.env$Model == kModel[2]) {
      stat <- "genos"
    }
    region.ids <- NULL
    if (file.exists(config.env$Labels.Path)) {
      # load the region labels reference file, renaming ABA-style column names
      # to the convention used in MagellanMapper
      region.ids <- read.csv(config.env$Labels.Path)
      region.ids <- renameCol(region.ids, "id", "Region")
      region.ids <- renameCol(region.ids, "name", "RegionName")
    }
    
    # reset graphics to ensure consistent layout
    while (!is.null(dev.list())) dev.off()
    
    # load CSV file output by MagellanMapper Python stats module
    message("Loading volume stats CSV: ", path)
    df <- read.csv(path)
    print(df)
    
    for (meas in measurements.split) {
      print(paste("Calculating stats for", meas))
      # set up output path for stats CSV file, creating directories if necessary
      path.out <- file.path(config.env$Prefix, paste0(
        kStatsPathOut, "_", meas, ".csv"))
      path.dir <- dirname(path.out)
      if (!file.exists(path.dir)) {
        dir.create(path.dir, recursive=TRUE)
      }
      if (load.stats && file.exists(path.out)) {
        # retrieve stats from a file to regenerate volcano plots
        stats <- read.csv(path.out)
      } else {
        # calculate stats
        stats <- calcVolStats(
          df, path.out, meas, config.env$Model, region.ids,
          split.by.side=config.env$Split.By.Side, corr=config.env$P.Corr)
      }
      
      if (!is.null(stats) & config.env$PlotVolcano) {
        # plot effects and p's
        volcanoPlot(stats, meas, stat, c(NA, 1.3, NA), config.env$VolcanoLogX, 
                    config.env$VolcanoLabels, config.env$PlotSize, 
                    meas.names=kMeasNames)
        volcanoPlot(stats, meas, "sidesR", c(25, 2.5, 0.2), 
                    config.env$VolcanoLogX, config.env$VolcanoLabels, 
                    config.env$PlotSize, meas.names=kMeasNames)
        # ":" special character automatically changed to "."
        volcanoPlot(stats, meas, paste0(stat, ".sidesR"), c(1e-04, 25, 0.2), 
                    config.env$VolcanoLogX, config.env$VolcanoLabels, 
                    config.env$PlotSize, meas.names=kMeasNames)
      }
    }
    
  } else if (stat.type == kStatTypes[2]) {
    # correlation coefficient matrix; 
    # TODO: generalize scenarios
    #calcCorr(config.env$StatsPathIn, kMeas[c(4:6, 10)], config.env$PlotSize)
    
    # intensity vs. nuclei from atlas regions
    calcCorr("../vols_stats_intensVnuc.csv", 
             c("Intensity.original", "Intensity.smoothed", 
               "Nuclei.original", "Nuclei.smoothed"), 
             config.env$PlotSize)
    calcCorr("../vols_stats_intensVnuc.csv", 
             c("Intensity_density.original", 
               "Intensity_density.smoothed", 
               "Nuclei_density.original", "Nuclei_density.smoothed"), 
             config.env$PlotSize, "density")
    
    # from ROIs
    calcCorr("../vols_stats_intensVnuc_rois_combined_condtocol.csv", 
             c("Intensity_detected", "Intensity_truth", 
               "Nuclei_detected", "Nuclei_truth"), 
             config.env$PlotSize)
    
  } else if (stat.type == kStatTypes[3]) {
    # normality test
    calcNormality(config.env$StatsPathIn, config.env$Measurements)
  }
}
